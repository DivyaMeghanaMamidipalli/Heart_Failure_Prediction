# -*- coding: utf-8 -*-
"""prml_project_heart_failure_prediction (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jTPNrOa-6U2_7fuX8-wASzuQ_R8sTCx_

#Project

##Importing required libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import RFE

from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import plot_roc_curve,roc_auc_score,roc_curve

import pickle

import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import OneHotEncoder , LabelEncoder , label_binarize
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score,confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import ExtraTreesClassifier

"""##Loading dataset"""

heart= pd.read_csv("https://raw.githubusercontent.com/mruganshi/MLP-from-Scratch/master/heart.csv")

heart

"""# Data Preprocessing

##Converting the chest pain type encoded column to the real name of the chest pain
"""

def chest_pain_type(title):
    if title =='ATA':
        return 'Atypical Angina'
    elif title=='TA':
        return 'Typical Angina'
    elif title=='NAP':
        return 'Non-Anginal Pain'
    elif title=='ASY':
        return 'Asymptomatic'
    else:
        return 'N/A'

heart['ChestPainType']=heart['ChestPainType'].apply(chest_pain_type)

"""##Checking for any null,NaN values"""

heart.info()

heart.isna().sum()

"""##Plotting and visualizing of data"""

sns.pairplot(heart,hue="HeartDisease")

sns.pairplot(heart, hue="HeartDisease", diag_kind="hist",kind="kde")

"""### Count Plot for different features in the dataset

feature : sex
"""

fig=plt.figure(figsize=(11,6))
sns.countplot(x=heart.Sex)

"""feature: chest pain type"""

fig=plt.figure(figsize=(11,6))
sns.countplot(x=heart.ChestPainType)

"""feature: resting ECG"""

fig=plt.figure(figsize=(11,6))
sns.countplot(x=heart.RestingECG)

"""feature: exercise angina"""

fig=plt.figure(figsize=(11,6))
sns.countplot(x=heart.ExerciseAngina)

"""feature: st_slope"""

fig=plt.figure(figsize=(11,6))
sns.countplot(x=heart.ST_Slope)

"""### Class-wise count plot for different features in the dataset

feature: sex
"""

fig=plt.figure(figsize=(11,6))
px.histogram(x=heart.Sex,color=heart.HeartDisease,barmode='group')

"""feature: chest pain type"""

fig=plt.figure(figsize=(11,6))
px.histogram(x=heart.ChestPainType,color=heart.HeartDisease,barmode='group')

"""feature: resting Ecg"""

fig=plt.figure(figsize=(11,6))
px.histogram(x=heart.RestingECG,color=heart.HeartDisease,barmode='group')

"""feature: exercise angina"""

fig=plt.figure(figsize=(11,6))
px.histogram(x=heart.ExerciseAngina,color=heart.HeartDisease,barmode='group')

"""feature: st_slope"""

fig=plt.figure(figsize=(11,6))
px.histogram(x=heart.ST_Slope,color=heart.HeartDisease,barmode='group')

"""### Age-wise frequency count for heart disease absent and heart disease present"""

k = np.unique(heart['Age'])

l_0 = []
l_1 = []
d=heart.iloc[:,[11]]
e=heart.iloc[:,[0]]
d=d.to_numpy()
e=e.to_numpy()
for j in k:
  p=[]
  count_0=0
  count_1=0
  for i in range(len(d)):
    if e[i]==j and d[i]==0:
      count_0+=1
    elif e[i]==j and d[i]==1:
      count_1+=1
  l_0.append(count_0)
  l_1.append(count_1)

fig = plt.subplots(figsize =(20, 8))
barWidth = 0.25
br1 = np.arange(len(l_0))
br2 = [x + barWidth for x in br1]

plt.bar(br1, l_0, color ='r', width = barWidth,
        edgecolor ='grey', label ='no_heart_disease')
plt.bar(br2,l_1, color ='g', width = barWidth,
        edgecolor ='grey', label ='yes_heart_disease')

"""### checking correlation of dependent and independent features"""

names=heart.columns
#Set the width and height of the plot
f, ax = plt.subplots(figsize=(10, 10))

#Correlation plot
df_corr = heart.loc[:,names]
#Generate correlation matrix
corr = df_corr.corr()

#Plot using seaborn library
sns.heatmap(corr, annot = True, cmap='coolwarm',linewidths=.1)
plt.show()

"""###Ordinal encoding of columns with feature values of object type"""

from sklearn.preprocessing import OrdinalEncoder
orcode = OrdinalEncoder()

heart[['Sex','ChestPainType','RestingECG','ExerciseAngina','ST_Slope']]= orcode.fit_transform(heart[['Sex','ChestPainType','RestingECG','ExerciseAngina','ST_Slope']])

heart

"""###Density Plot for numeric features"""

cols=['Age','RestingBP','Cholesterol','FastingBS','MaxHR','Oldpeak']

for c in cols:
    plt.figure(figsize=(7,7))
    sns.distplot(heart.loc[heart['HeartDisease']==1][c],kde_kws={'label':'Heart Disease'},color='green')
    sns.distplot(heart.loc[heart['HeartDisease']==0][c],kde_kws={'label':'Normal'},color='red')
    
    plt.legend(labels=['Normal','Heart Disease'])
    plt.title(c)

"""###Box Plots to visualize outliers"""

for c in names:
    fig=plt.figure(figsize=(9,6))
    sns.boxplot(x=heart[c],hue=heart.HeartDisease)

"""##Splitting the data into train and test data"""

from sklearn.model_selection import train_test_split
x=heart.drop(columns=['HeartDisease'],axis=1)
y=heart['HeartDisease']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)

len(x_train)

len(x_test)

len(y_train)

len(y_test)

"""# Model fitting to predict heart failure

### Decision Tree
"""

clf = DecisionTreeClassifier(random_state = 0)
clf.fit(x_train, y_train)
test_pred_decision_tree = clf.predict(x_test)
acc_decision_tree=accuracy_score(y_test,test_pred_decision_tree)*100
acc_decision_tree

"""### KNN"""

neigh = KNeighborsClassifier()
neigh.fit(x_train, y_train)
test_pred_knn = neigh.predict(x_test)
acc_knn=accuracy_score(y_test,test_pred_knn)*100
acc_knn

"""### Gradient boosting """

gbc = GradientBoostingClassifier()
gbc.fit(x_train,y_train)
test_pred_gbc = gbc.predict(x_test)
acc_gbc=accuracy_score(y_test,test_pred_gbc)*100
acc_gbc

"""### Gaussian Naivebayes"""

gnb = GaussianNB()
gnb.fit(x_train,y_train)
test_pred_gnb = gnb.predict(x_test)
acc_gnb=accuracy_score(y_test,test_pred_gnb)*100
acc_gnb

"""### Logistic Regression"""

lr= LogisticRegression()
lr.fit(x_train,y_train)
test_pred_lr = lr.predict(x_test)
acc_lr=accuracy_score(y_test,test_pred_lr)*100
acc_lr

"""### Multilayer Perceptron"""

from sklearn.neural_network import MLPClassifier
percep = MLPClassifier()
percep.fit(x_train,y_train)
test_pred_percep = lr.predict(x_test)
acc_percep=accuracy_score(y_test,test_pred_percep)*100
acc_percep

"""### K-means"""

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=2)
kmeans.fit(x_train,y_train)
test_pred_km = kmeans.predict(x_test)
acc_km=accuracy_score(y_test,test_pred_km)*100
acc_km

"""### Random Forest Regression"""

from sklearn.ensemble import RandomForestClassifier
regr = RandomForestClassifier()
regr.fit(x_train,y_train)
test_pred_regr = regr.predict(x_test)
acc_regr=accuracy_score(y_test,test_pred_regr)*100
acc_regr

"""### Comparison"""

Models1=pd.DataFrame({'Models':['Decision Tree Classifier','KNeighbors','GradientBoosting','Gaussian Naivebayes','Logistic Regression','Multilayer Perceptron','K-means','Random forest regressor'],
                     'accuracy_Score':[acc_decision_tree,acc_knn,acc_gbc,acc_gnb,acc_lr,acc_percep,acc_km,acc_regr]
                    })
Models1

"""# Hyperparameter tuning

### Decision Tree
"""

from sklearn.metrics import mean_squared_error as mse

max_possible_depths=range(1,20)   #varying parameter max_depth

training=[]
for max_depth in max_possible_depths:
  m1=DecisionTreeClassifier(max_depth=max_depth)
  m1.fit(x_train,y_train)
  training.append(mse(y_train,m1.predict(x_train)))

testing=[]
for max_depth in max_possible_depths:
  m2=DecisionTreeClassifier(max_depth=max_depth)
  m2.fit(x_test,y_test)
  testing.append(mse(y_test,m2.predict(x_test)))

plt.plot(max_possible_depths, training, color='red', label='Training Error')
plt.plot(max_possible_depths, testing, color='blue', label='Testing Error')

min_possible_samples=range(2,20)         #varying parameter min_samples_split

training=[]
for min_sample in min_possible_samples:
  m1=DecisionTreeClassifier(min_samples_split=min_sample)
  m1.fit(x_train,y_train)
  training.append(mse(y_train,m1.predict(x_train)))

testing=[]
for min_sample in min_possible_samples:
  m2=DecisionTreeClassifier(min_samples_split=min_sample)
  m2.fit(x_test,y_test)
  testing.append(mse(y_test,m2.predict(x_test)))

plt.plot(min_possible_samples, training, color='red', label='Training Error')
plt.plot(min_possible_samples, testing, color='blue', label='Testing Error')

clf2 = DecisionTreeClassifier(random_state = 0,max_depth=9,min_samples_split=8)           #making predictions on x_test using the best parameters obtained
clf2.fit(x_train, y_train)
test_pred_decision_tree2 = clf2.predict(x_test)
acc_decision_tree2=accuracy_score(y_test,test_pred_decision_tree2)*100
acc_decision_tree2

"""### KNN"""

from sklearn.model_selection import GridSearchCV                               #finding best parameters using gridsearchCV for different parameters
grid_params = { 'n_neighbors' : [i for i in range(3,20)],
               'weights' : ['uniform','distance'],
               'metric' : ['minkowski','euclidean','manhattan']}
gs = GridSearchCV(KNeighborsClassifier(), grid_params, verbose = 1, cv=7, n_jobs = -1)
g_res = gs.fit(x_train, y_train)
knn_pred = g_res.predict(x_test)
knn_grid = accuracy_score(y_test,knn_pred)*100
print(knn_grid)
g_res.best_params_

"""### Gradient boosting"""

param_test2 = {'max_depth':range(5,16,2), 'min_samples_split':range(200,1001,200)}           #finding best parameters using gridsearchCV for different parameters
gsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60, max_features='sqrt', subsample=0.8, random_state=10),param_grid = param_test2, scoring='roc_auc',n_jobs=4, cv=5)
g_res_grad = gsearch2.fit(x_train, y_train)
gb_pred = g_res_grad.predict(x_test)
gb_grid = accuracy_score(y_test,gb_pred)*100
g_res.best_params_
gb_grid

"""### Gaussian Naivebayes"""

param_grid_nb = {                                                                   #setting the parameters to choose from
    'var_smoothing': np.logspace(0,-9, num=100)
}

from sklearn.naive_bayes import GaussianNB                                             #finding best parameters using gridsearchCV for different parameters
from sklearn.model_selection import GridSearchCV
nbModel_grid = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, verbose=1, cv=10, n_jobs=-1)
nbModel_grid.fit(x_train, y_train)
gn_pred = nbModel_grid.predict(x_test)
gn_grid = accuracy_score(y_test,gn_pred)*100
gn_grid

"""### Logistic Regression"""

from sklearn.model_selection import RepeatedStratifiedKFold
model = LogisticRegression()                                                                 #finding best parameters using gridsearchCV for different parameters
solvers = ['newton-cg', 'lbfgs', 'liblinear']
penalty = ['l2']
c_values = [100, 10, 1.0, 0.1, 0.01]
# define grid search
grid = dict(solver=solvers,penalty=penalty,C=c_values)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
grid_result = grid_search.fit(x_train, y_train)
grid_pred = grid_result.predict(x_test)
lr_grid = accuracy_score(y_test,grid_pred)*100
lr_grid

"""### Multilayer Perceptron"""

parameter_space = {
    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],                            #setting the parameters to choose from
    'activation': ['tanh', 'relu'],
    'solver': ['sgd', 'adam'],
    'alpha': [0.0001, 0.05],
    'learning_rate': ['constant','adaptive'],
}

from sklearn.model_selection import GridSearchCV
from sklearn.neural_network import MLPClassifier
mlp = MLPClassifier(max_iter=1000)                                          #finding best parameters using gridsearchCV for different parameters
mp = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=5)
mp.fit(x_train,y_train)
mppp = mp.predict(x_test)
mpp = accuracy_score(y_test,mppp)*100
mpp

"""### K-means"""

i= ['k-means++', 'random']
                                                              #varying parameter init
training=[]
for c in i:
  m1=KMeans(init=c)
  m1.fit(x_train,y_train)
  training.append(mse(y_train,m1.predict(x_train)))

testing=[]
for c in i:
  m2=KMeans(init=c)
  m2.fit(x_test,y_test)
  testing.append(mse(y_test,m2.predict(x_test)))

plt.plot(i, training, color='red', label='Training Error')
plt.plot(i, testing, color='blue', label='Testing Error')

clf4 = KMeans(n_clusters=2,init='k-means++')                     #making predictions on x_test using the best parameters obtained
clf4.fit(x_train, y_train)
test_pred4 = clf4.predict(x_test)
acc4=accuracy_score(y_test,test_pred4)*100
acc4

"""### Random forest regressor"""

max_possible_depths=range(1,20)                                    #varying parameter max_depth

training=[]
for max_depth in max_possible_depths:
  m1=RandomForestClassifier(max_depth=max_depth)
  m1.fit(x_train,y_train)
  training.append(mse(y_train,m1.predict(x_train)))

testing=[]
for max_depth in max_possible_depths:
  m2=RandomForestClassifier(max_depth=max_depth)
  m2.fit(x_test,y_test)
  testing.append(mse(y_test,m2.predict(x_test)))

plt.plot(max_possible_depths, training, color='red', label='Training Error')
plt.plot(max_possible_depths, testing, color='blue', label='Testing Error')

min_possible_samples=range(2,20)                                 #varying parameter min_samples_split

training=[]
for min_sample in min_possible_samples:
  m1=RandomForestClassifier(min_samples_split=min_sample)
  m1.fit(x_train,y_train)
  training.append(mse(y_train,m1.predict(x_train)))

testing=[]
for min_sample in min_possible_samples:
  m2=RandomForestClassifier(min_samples_split=min_sample)
  m2.fit(x_test,y_test)
  testing.append(mse(y_test,m2.predict(x_test)))

plt.plot(min_possible_samples, training, color='red', label='Training Error')
plt.plot(min_possible_samples, testing, color='blue', label='Testing Error')

min_possible_leaf=range(1,20)                                            #varying parameter min_samples_leaf

training=[]
for min_sample in min_possible_leaf:
  m1=RandomForestClassifier(min_samples_leaf=min_sample)
  m1.fit(x_train,y_train)
  training.append(mse(y_train,m1.predict(x_train)))

testing=[]
for min_sample in min_possible_leaf:
  m2=RandomForestClassifier(min_samples_leaf=min_sample)
  m2.fit(x_test,y_test)
  testing.append(mse(y_test,m2.predict(x_test)))

plt.plot(min_possible_leaf, training, color='red', label='Training Error')
plt.plot(min_possible_leaf, testing, color='blue', label='Testing Error')

clf3 = RandomForestClassifier(random_state = 0,max_depth=10,min_samples_split=10,min_samples_leaf=3)         #making predictions on x_test using the best parameters obtained
clf3.fit(x_train, y_train)
test_pred3 = clf3.predict(x_test)
acc3=accuracy_score(y_test,test_pred3)*100
acc3

"""### comparing accuracy after hyperparameter tuning"""

plot = ["before","after"]

"""decision tree"""

dt=[acc_decision_tree,acc_decision_tree2]
fig=plt.figure(figsize=(5,5))
sns.barplot(x=plot,y=dt)
plt.xticks(rotation=90)
fig.show()

"""KNeighbors"""

kn=[acc_knn,knn_grid]
fig=plt.figure(figsize=(5,5))
sns.barplot(x=plot,y=kn)
plt.xticks(rotation=90)
fig.show()

"""GradientBoosting"""

gb=[acc_gbc,gb_grid]
fig=plt.figure(figsize=(5,5))
sns.barplot(x=plot,y=gb)
plt.xticks(rotation=90)
fig.show()

"""Gaussian Naivebayes"""

gn=[acc_gnb,gn_grid]
fig=plt.figure(figsize=(5,5))
sns.barplot(x=plot,y=gn)
plt.xticks(rotation=90)
fig.show()

"""Logistic Regression"""

lr=[acc_lr,lr_grid]
fig=plt.figure(figsize=(5,5))
sns.barplot(x=plot,y=lr)
plt.show()

"""Multilayer Perceptron"""

m=[acc_percep,mpp]
fig=plt.figure(figsize=(5,5))
sns.barplot(x=plot,y=m)
plt.xticks(rotation=90)
fig.show()

"""K-means"""

km=[acc_km,acc4]
fig=plt.figure(figsize=(5,5))
sns.barplot(x=plot,y=km)
plt.xticks(rotation=90)
fig.show()

"""Random forest regressor"""

rf=[acc_regr,acc3]
fig=plt.figure(figsize=(5,5))
sns.barplot(x=plot,y=rf)
plt.xticks(rotation=90)
fig.show()

"""## Comparison"""

Models=pd.DataFrame({'Models':['Decision Tree Classifier','KNeighbors','GradientBoosting','Gaussian Naivebayes','Logistic Regression','Multilayer Perceptron','K-means','Random forest regressor'],
                    'Final_Score':[acc_decision_tree2,knn_grid,gb_grid,gn_grid,lr_grid,mpp,acc4,acc3],
                     'Initial_Score':[acc_decision_tree,acc_knn,acc_gbc,acc_gnb,acc_lr,acc_percep,acc_km,acc_regr]
                    })

Models

fig=plt.figure(figsize=(5,5))
sns.barplot(x=Models.Models,y=Models.Final_Score)
plt.xticks(rotation=90)
fig.show()

"""### ROC

decision tree classifier
"""

r_fpr,r_tpr,_=roc_curve(y_test,test_pred_decision_tree2)
r_auc=roc_auc_score(y_test,test_pred_decision_tree2)
plt.plot(r_fpr,r_tpr,label='Decision Tree Classifier (area={:.3f})'.format(r_auc))
plt.title('ROC plot Decision Tree Classifier')
plt.xlabel('False Positive rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.show()

"""knn"""

r_fpr,r_tpr,_=roc_curve(y_test,g_res.predict(x_test))
r_auc=roc_auc_score(y_test,g_res.predict(x_test))
plt.plot(r_fpr,r_tpr,label='KNeighbors (area={:.3f})'.format(r_auc))
plt.title('ROC plot KNeighbors')
plt.xlabel('False Positive rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.show()

"""GradientBoosting"""

r_fpr,r_tpr,_=roc_curve(y_test,g_res_grad.predict(x_test))
r_auc=roc_auc_score(y_test,g_res_grad.predict(x_test))
plt.plot(r_fpr,r_tpr,label='GradientBoosting (area={:.3f})'.format(r_auc))
plt.title('ROC plot GradientBoosting')
plt.xlabel('False Positive rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.show()

"""Gaussian Naivebayes"""

r_fpr,r_tpr,_=roc_curve(y_test,nbModel_grid.predict(x_test))
r_auc=roc_auc_score(y_test,nbModel_grid.predict(x_test))
plt.plot(r_fpr,r_tpr,label='Gaussian Naivebayes (area={:.3f})'.format(r_auc))
plt.title('ROC plot Gaussian Naivebayes')
plt.xlabel('False Positive rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.show()

"""Logistic Regression"""

r_fpr,r_tpr,_=roc_curve(y_test,grid_result.predict(x_test))
r_auc=roc_auc_score(y_test,grid_result.predict(x_test))
plt.plot(r_fpr,r_tpr,label='Logistic Regression (area={:.3f})'.format(r_auc))
plt.title('ROC plot Logistic Regression')
plt.xlabel('False Positive rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.show()

"""Multilayer Perceptron"""

r_fpr,r_tpr,_=roc_curve(y_test,mp.predict(x_test))
r_auc=roc_auc_score(y_test,mp.predict(x_test))
plt.plot(r_fpr,r_tpr,label='Multilayer Perceptron (area={:.3f})'.format(r_auc))
plt.title('ROC plot Multilayer Perceptron')
plt.xlabel('False Positive rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.show()

"""K-means"""

r_fpr,r_tpr,_=roc_curve(y_test,test_pred4)
r_auc=roc_auc_score(y_test,test_pred4)
plt.plot(r_fpr,r_tpr,label='K-means (area={:.3f})'.format(r_auc))
plt.title('ROC plot K-means')
plt.xlabel('False Positive rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.show()

"""Random forest regressor"""

r_fpr,r_tpr,_=roc_curve(y_test,test_pred3)
r_auc=roc_auc_score(y_test,test_pred3)
plt.plot(r_fpr,r_tpr,label='Random forest regressor (area={:.3f})'.format(r_auc))
plt.title('ROC plot Random forest regressor')
plt.xlabel('False Positive rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.show()

"""# classification report

decision tree
"""

print(classification_report(y_test,test_pred_decision_tree2))

"""knn"""

print(classification_report(y_test,g_res.predict(x_test)))

"""gradient boosting"""

print(classification_report(y_test,g_res_grad.predict(x_test)))

"""gaussian naive bayes"""

print(classification_report(y_test,nbModel_grid.predict(x_test)))

"""logistic regression"""

print(classification_report(y_test,grid_result.predict(x_test)))

"""multilayer perceptron"""

print(classification_report(y_test,mp.predict(x_test)))

"""k-means"""

print(classification_report(y_test,test_pred4))

"""random forest regressor"""

print(classification_report(y_test,test_pred3))

"""# Cross Validation

### Decision tree classifier
"""

decision_tree=[]
for i in range(2,9):
  cv=cross_val_score(clf2,x_train,y_train,cv=i)
  decision_tree.append(cv.mean())

"""### Kneighbours"""

kneighbours=[]
for i in range(2,9):
  cv=cross_val_score(g_res,x_train,y_train,cv=i)
  kneighbours.append(cv.mean())

"""### Guassian naive bayes"""

guassian=[]
for i in range(2,9):
  cv=cross_val_score(nbModel_grid,x_train,y_train,cv=i)
  guassian.append(cv.mean())

"""### Comparison"""

import matplotlib.pyplot as plt
plt.figure(figsize=(6,6))
x=[i for i in range(2,9)]
plt.plot(x,decision_tree,color = 'Blue',label='Decision Tree')
plt.plot(x,kneighbours,color = 'Red',label='KNN')
plt.plot(x,guassian,color = 'Green',label='Gaussian Naive Bayes')
plt.ylabel("Mean CV score")
plt.xlabel("No. of folds")
plt.title('Comparison of all models using mean CV scores', size=15)
plt.legend()
plt.show()